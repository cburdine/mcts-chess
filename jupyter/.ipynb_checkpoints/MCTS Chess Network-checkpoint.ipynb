{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ada7e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-13 11:55:44.951968: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-05-13 11:55:44.951988: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "hindu-reliance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF Version: 2.8.0\n"
     ]
    }
   ],
   "source": [
    "print('TF Version:', tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30c620c",
   "metadata": {},
   "source": [
    "## MCTS Chess Neural Network\n",
    "---\n",
    "\n",
    "[notes go here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fe4a990",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "\n",
    "class SimpleChessNet(Model):\n",
    "    \n",
    "    def __init__(self, action_size,\n",
    "                 board_input_shape=(8,8,6),\n",
    "                 n_conv_channels=12,\n",
    "                 dropout_rate=0.5):\n",
    "        \n",
    "        super(SimpleChessNet, self).__init__()\n",
    "        \n",
    "        self.board_x, self.board_y, self.board_channels = board_input_shape\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        self.input_layer = Input(shape=board_input_shape)\n",
    "        \n",
    "        h_conv1 = Activation('elu')(BatchNormalization(axis=3)(Conv2D(n_conv_channels, 3, padding='same', use_bias=False)(self.input_layer))) # batch_size  x board_x x board_y x num_channels\n",
    "        h_conv2 = Activation('elu')(BatchNormalization(axis=3)(Conv2D(n_conv_channels, 3, padding='same', use_bias=False)(h_conv1)))          # batch_size  x board_x x board_y x num_channels\n",
    "        h_conv3 = Activation('elu')(BatchNormalization(axis=3)(Conv2D(n_conv_channels, 3, padding='valid', use_bias=False)(h_conv2)))         # batch_size  x (board_x-2) x (board_y-2) x num_channels\n",
    "        h_conv4 = Activation('elu')(BatchNormalization(axis=3)(Conv2D(n_conv_channels, 3, padding='valid', use_bias=False)(h_conv3)))         # batch_size  x (board_x-4) x (board_y-4) x num_channels\n",
    "        h_conv4_flat = Flatten()(h_conv4)\n",
    "        \n",
    "        s_fc1 = Dropout(dropout_rate)(Activation('elu')(BatchNormalization(axis=1)(Dense(1024, use_bias=False)(h_conv4_flat))))  # batch_size x 1024\n",
    "        s_fc2 = Dropout(dropout_rate)(Activation('elu')(BatchNormalization(axis=1)(Dense(512, use_bias=False)(s_fc1))))          # batch_size x 1024\n",
    "        \n",
    "        # get the policy and value estimates:\n",
    "        self.pi = Dense(self.action_size, activation='softmax', name='pi')(s_fc2) # batch_size x self.action_size\n",
    "        self.v = Dense(1, activation='tanh', name='v')(s_fc2)\n",
    "        \n",
    "        self.model = Model(inputs=self.input_layer, outputs=[self.pi, self.v])\n",
    "        # self.model.compile(loss=['categorical_crossentropy', 'mean_squared_error'], optimizer=Adam(learning_rate))\n",
    "        \n",
    "    def call(self, x):\n",
    "        return self.model(x)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return self.model(x)\n",
    "        \n",
    "    def compute_loss(self, x, pi, v):\n",
    "        policy_pred, v_pred = self.__call__(x)\n",
    "        v_loss = tf.reduce_mean((v_pred - v)**2)\n",
    "        pi_loss = tf.reduce_mean(-pi*tf.math.log(policy_pred))\n",
    "        total_loss = v_loss + pi_loss #tf.add_n([(v_loss + pi_loss)] + self.losses)\n",
    "        return v_loss, pi_loss, total_loss\n",
    "    \n",
    "    def save(self, path):\n",
    "        tf.saved_model.save(self.model, path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67e41a68",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 8, 8, 6)]    0           []                               \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 8, 8, 12)     648         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 8, 8, 12)    48          ['conv2d[0][0]']                 \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 8, 8, 12)     0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 8, 8, 12)     1296        ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 8, 8, 12)    48          ['conv2d_1[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 8, 8, 12)     0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 6, 6, 12)     1296        ['activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 6, 6, 12)    48          ['conv2d_2[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 6, 6, 12)     0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 4, 4, 12)     1296        ['activation_2[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 4, 4, 12)    48          ['conv2d_3[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 4, 4, 12)     0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 192)          0           ['activation_3[0][0]']           \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 1024)         196608      ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 1024)        4096        ['dense[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 1024)         0           ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 1024)         0           ['activation_4[0][0]']           \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 512)          524288      ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 512)         2048        ['dense_1[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (None, 512)          0           ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 512)          0           ['activation_5[0][0]']           \n",
      "                                                                                                  \n",
      " pi (Dense)                     (None, 4096)         2101248     ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " v (Dense)                      (None, 1)            513         ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,833,529\n",
      "Trainable params: 2,830,361\n",
      "Non-trainable params: 3,168\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-13 11:55:46.303131: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-05-13 11:55:46.303163: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-05-13 11:55:46.303181: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (arcturus): /proc/driver/nvidia/version does not exist\n",
      "2022-05-13 11:55:46.303423: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "nn = SimpleChessNet(action_size=64*64)\n",
    "nn.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1b453b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import *\n",
    "\n",
    "class ChessNetModule(tf.Module):\n",
    "    \n",
    "    def __init__(self, learning_rate=1e-5):\n",
    "        super(ChessNetModule, self).__init__()\n",
    "        self.model = SimpleChessNet(action_size=64*64)\n",
    "        self.optimizer = Adam(learning_rate)\n",
    "        self.checkpoint = tf.train.Checkpoint(self.model)\n",
    "        \n",
    "        \n",
    "        \n",
    "    @tf.function(input_signature=[tf.TensorSpec([None,8,8,6])])\n",
    "    def __call__(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    @tf.function(input_signature=[tf.TensorSpec([None,8,8,6], tf.float32), \n",
    "                                  tf.TensorSpec([None,64*64], tf.float32), \n",
    "                                  tf.TensorSpec([None], tf.float32)])\n",
    "    def train_step(self, x, pi, v):\n",
    "        with tf.GradientTape() as tape:\n",
    "            v_loss, pi_loss, total_loss = self.model.compute_loss(x, pi, v)\n",
    "        \n",
    "        grads = tape.gradient(total_loss, self.model.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_weights))\n",
    "        \n",
    "        return v_loss, pi_loss, total_loss\n",
    "    \n",
    "    @tf.function(input_signature=[tf.TensorSpec([None,8,8,6], tf.float32), \n",
    "                                  tf.TensorSpec([None,64*64], tf.float32), \n",
    "                                  tf.TensorSpec([None], tf.float32)])\n",
    "    def validate_step(self, x, pi, v):\n",
    "            v_loss, pi_loss, total_loss = self.model.compute_loss(x, pi, v)\n",
    "            return v_loss, pi_loss, total_loss\n",
    "    \n",
    "    @tf.function(input_signature=[tf.TensorSpec([], tf.string)])\n",
    "    def save_checkpoint(self, path):\n",
    "        self.checkpoint.write(path)\n",
    "        return path\n",
    "    \n",
    "    @tf.function(input_signature=[tf.TensorSpec([], tf.string)])\n",
    "    def reset_optimizer(self, options):\n",
    "        for var in self.optimizer.variables():\n",
    "            var.assign(tf.zeros_like(var))\n",
    "        return options\n",
    "    \n",
    "    def save(self, path):\n",
    "        tf.saved_model.save(self, path, \n",
    "        signatures={\n",
    "            'chessnet_serve' : \n",
    "                self.__call__.get_concrete_function(tf.TensorSpec([None,8,8,6], tf.float32)),\n",
    "            'chessnet_train' : \n",
    "                self.train_step.get_concrete_function(tf.TensorSpec([None,8,8,6], tf.float32),\n",
    "                                                      tf.TensorSpec([None,64*64], tf.float32),\n",
    "                                                      tf.TensorSpec([None], tf.float32)),\n",
    "            'chessnet_validate' : \n",
    "                self.validate_step.get_concrete_function(tf.TensorSpec([None,8,8,6], tf.float32),\n",
    "                                                      tf.TensorSpec([None,64*64], tf.float32),\n",
    "                                                      tf.TensorSpec([None], tf.float32)),\n",
    "            'chessnet_save_checkpoint':\n",
    "                self.save_checkpoint.get_concrete_function(tf.TensorSpec([], tf.string)),\n",
    "            'chessnet_reset_optimizer':\n",
    "                self.reset_optimizer.get_concrete_function(tf.TensorSpec([], tf.string)),\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e92ae11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-13 11:55:47.619754: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.SimpleChessNet object at 0x7f5e8cf70790>, because it is not built.\n",
      "INFO:tensorflow:Assets written to: simple_chess_net/assets\n"
     ]
    }
   ],
   "source": [
    "nn_mod = ChessNetModule()\n",
    "nn_mod.save('simple_chess_net')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a41118c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
